---
author: nido
categories: 技术
comments_data:
- date: '2015-03-21 18:03:01'
  message: get~
  postip: 68.180.228.107
  username: 微博评论
- date: '2015-03-21 18:03:01'
  message: Repost
  postip: 68.180.228.107
  username: 微博评论
- date: '2015-03-21 18:03:01'
  message: '@mywiz'
  postip: 68.180.228.107
  username: 微博评论
- date: '2015-03-21 18:03:01'
  message: 人生苦短，我用pyspider
  postip: 68.180.228.107
  username: 微博评论
- date: '2015-03-21 18:15:03'
  message: 还处在requests阶段，刚刚开始玩BeautifulSoup4，因为我实在不想学Python2，从开始玩就是Python3，觉得舒服一点，反正不是靠这个吃饭的
  postip: 121.34.172.213
  username: 来自 - 广东深圳 的 Firefox/Windows 用户
- date: '2015-03-21 18:33:01'
  message: '@保存到为知笔记'
  postip: 119.85.112.84
  username: 微博评论
- date: '2015-03-21 18:33:01'
  message: '[给力]'
  postip: 119.85.112.84
  username: 微博评论
- date: '2015-03-21 19:03:01'
  message: 没事玩玩
  postip: 69.197.182.250
  username: 微博评论
- date: '2015-03-21 19:03:01'
  message: 先收着
  postip: 69.197.182.250
  username: 微博评论
- date: '2015-03-21 19:33:01'
  message: scrapy
  postip: 42.156.139.52
  username: 微博评论
- date: '2015-03-21 19:33:01'
  message: Repost
  postip: 42.156.139.52
  username: 微博评论
- date: '2015-03-21 20:03:01'
  message: 断断续续的，我已经做了三周了。
  postip: 42.249.138.179
  username: 微博评论
- date: '2015-03-21 20:03:01'
  message: mark
  postip: 42.249.138.179
  username: 微博评论
- date: '2015-03-21 22:03:03'
  message: 转发微博.
  postip: 183.34.167.234
  username: 微博评论
- date: '2015-03-21 22:33:01'
  message: 看看
  postip: 66.249.79.111
  username: 微博评论
- date: '2015-03-21 23:03:06'
  message: '@我的印象笔记'
  postip: 183.34.167.234
  username: 微博评论
- date: '2015-03-22 00:33:02'
  message: '@我的印象笔记'
  postip: 207.241.237.228
  username: 微博评论
- date: '2015-03-22 05:03:01'
  message: 马克
  postip: 36.248.166.210
  username: 微博评论
- date: '2015-03-22 11:03:08'
  message: '@保存到为知笔记'
  postip: 119.167.246.16
  username: 微博评论
- date: '2015-03-22 17:33:01'
  message: '@我的印象笔记'
  postip: 42.120.160.52
  username: 微博评论
- date: '2015-03-22 18:33:01'
  message: '@我的印象笔记'
  postip: 202.112.129.242
  username: 微博评论
count:
  commentnum: 21
  favtimes: 7
  likes: 0
  sharetimes: 35
  viewnum: 13147
date: '2015-03-21 17:42:00'
editorchoice: false
excerpt: 这是一款提取网站数据的开源工具。Scrapy框架用Python开发而成，它使抓取工作又快又简单，且可扩展。我们已经在virtual box中创建一台虚拟机（VM）并且在上面安装了Ubuntu
  14.04 LTS。 安装 Scrapy  Scrapy依赖于Python、开发库和pip。Python最新的版本已经在Ubuntu上预装了。因此我们在安装Scrapy之前只需安装pip和python开发库就可以了。
  pip是作为python包索引器easy_install的替代品，用于安装和管理Python包。pip包的安装可见图 1。 sudo apt-get install
  python-pip   图:1 pip安装 我们必须要用下面的命令安装python开发库。如果包
fromurl: ''
id: 5098
islctt: true
largepic: /data/attachment/album/201503/21/174213pc6aqwfzf81qnzga.jpg
url: /article-5098-1.html
pic: /data/attachment/album/201503/21/174213pc6aqwfzf81qnzga.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: 这是一款提取网站数据的开源工具。Scrapy框架用Python开发而成，它使抓取工作又快又简单，且可扩展。我们已经在virtual box中创建一台虚拟机（VM）并且在上面安装了Ubuntu
  14.04 LTS。 安装 Scrapy  Scrapy依赖于Python、开发库和pip。Python最新的版本已经在Ubuntu上预装了。因此我们在安装Scrapy之前只需安装pip和python开发库就可以了。
  pip是作为python包索引器easy_install的替代品，用于安装和管理Python包。pip包的安装可见图 1。 sudo apt-get install
  python-pip   图:1 pip安装 我们必须要用下面的命令安装python开发库。如果包
tags:
- Python
- Scrapy
- 爬虫
thumb: false
title: 如何在Ubuntu 14.04 LTS安装网络爬虫工具：Scrapy
titlepic: false
translator: geekpi
updated: '2015-03-21 17:42:00'
---

这是一款提取网站数据的开源工具。Scrapy框架用Python开发而成，它使抓取工作又快又简单，且可扩展。我们已经在virtual box中创建一台虚拟机（VM）并且在上面安装了Ubuntu 14.04 LTS。


### 安装 Scrapy


![](/data/attachment/album/201503/21/174213pc6aqwfzf81qnzga.jpg)


Scrapy依赖于Python、开发库和pip。Python最新的版本已经在Ubuntu上预装了。因此我们在安装Scrapy之前只需安装pip和python开发库就可以了。


pip是作为python包索引器easy\_install的替代品，用于安装和管理Python包。pip包的安装可见图 1。



```
sudo apt-get install python-pip

```

![Fig:1 Pip installation](/data/attachment/album/201503/21/174230mh16gvlvhzzl6urt.png)


*图:1 pip安装*


我们必须要用下面的命令安装python开发库。如果包没有安装那么就会在安装scrapy框架的时候报关于python.h头文件的错误。



```
sudo apt-get install python-dev

```

![Fig:2 Python Developer Libraries](/data/attachment/album/201503/21/174231qahzo63dabnbaby8.png)


*图:2 Python 开发库*


scrapy框架既可从deb包安装也可以从源码安装。在图3中我们用pip（Python 包管理器）安装了deb包了。



```
sudo pip install scrapy 

```

![Fig:3 Scrapy Installation](/data/attachment/album/201503/21/174232nb8xy0ylvzb5umnt.png)


*图:3 Scrapy 安装*


图4中scrapy的成功安装需要一些时间。


![Fig:4 Successful installation of Scrapy Framework](/data/attachment/album/201503/21/174234aczsnbic07gsngss.png)


*图:4 成功安装Scrapy框架*


### 使用scrapy框架提取数据


#### 基础教程


我们将用scrapy从fatwallet.com上提取商店名称（卖卡的店）。首先，我们使用下面的命令新建一个scrapy项目“store name”， 见图5。



```
$sudo scrapy startproject store_name

```

![Fig:5 Creation of new project in Scrapy Framework](/data/attachment/album/201503/21/174237ldhdytd10yg44hyh.png)


*图:5 Scrapy框架新建项目*


上面的命令在当前路径创建了一个“store\_name”的目录。项目主目录下包含的文件/文件夹见图6。



```
$sudo ls –lR store_name

```

![Fig:6 Contents of store_name project.](/data/attachment/album/201503/21/174240dhzs3aa7df3s0shu.png)


*图:6 store\_name项目的内容*


每个文件/文件夹的概要如下：


* scrapy.cfg 是项目配置文件
* store\_name/ 主目录下的另一个文件夹。 这个目录包含了项目的python代码
* store\_name/items.py 包含了将由蜘蛛爬取的项目
* store\_name/pipelines.py 是管道文件
* store\_name/settings.py 是项目的配置文件
* store\_name/spiders/， 包含了用于爬取的蜘蛛


由于我们要从fatwallet.com上如提取店名，因此我们如下修改文件（LCTT 译注：这里没说明是哪个文件，译者认为应该是 items.py）。



```
import scrapy

class StoreNameItem(scrapy.Item):

   name = scrapy.Field()   #  取出卡片商店的名称

```

之后我们要在项目的store\_name/spiders/文件夹下写一个新的蜘蛛。蜘蛛是一个python类，它包含了下面几个必须的属性：


1. 蜘蛛名 (name )
2. 爬取起点url (start\_urls)
3. 包含了从响应中提取需要内容相应的正则表达式的解析方法。解析方法对爬虫而言很重要。


我们在store*name/spiders/目录下创建了“store*name.py”爬虫，并添加如下的代码来从fatwallet.com上提取店名。爬虫的输出写到文件（**StoreName.txt**）中，见图7。



```
from scrapy.selector import Selector
from scrapy.spider import BaseSpider
from scrapy.http import Request
from scrapy.http import FormRequest
import re
class StoreNameItem(BaseSpider):
name = "storename"
allowed_domains = ["fatwallet.com"]
start_urls = ["http://fatwallet.com/cash-back-shopping/"]

def parse(self,response):
output = open('StoreName.txt','w')
resp = Selector(response)

tags = resp.xpath('//tr[@class="storeListRow"]|\
         //tr[@class="storeListRow even"]|\
         //tr[@class="storeListRow even last"]|\
          //tr[@class="storeListRow last"]').extract()
for i in tags:
i = i.encode('utf-8', 'ignore').strip()
store_name = ''
if re.search(r"class=\"storeListStoreName\">.*?<",i,re.I|re.S):
store_name = re.search(r"class=\"storeListStoreName\">.*?<",i,re.I|re.S).group()
store_name = re.search(r">.*?<",store_name,re.I|re.S).group()
store_name = re.sub(r'>',"",re.sub(r'<',"",store_name,re.I))
store_name = re.sub(r'&amp;',"&",re.sub(r'&amp;',"&",store_name,re.I))
#print store_name
output.write(store_name+""+"\n")

```

![Fig:7 Output of the Spider code .](/data/attachment/album/201503/21/174242z4g4b5bxg4p9s4ga.png)


*图:7 爬虫的输出*


*注意: 本教程的目的仅用于理解scrapy框架*




---


via: <http://linoxide.com/ubuntu-how-to/scrapy-install-ubuntu/>


作者：[nido](http://linoxide.com/author/naveeda/) 译者：[geekpi](https://github.com/geekpi) 校对：[wxy](https://github.com/wxy)


本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创翻译，[Linux中国](http://linux.cn/) 荣誉推出