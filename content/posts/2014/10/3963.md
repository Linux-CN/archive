---
author: ertou
categories: 系统运维
comments_data: []
count:
  commentnum: 0
  favtimes: 18
  likes: 0
  sharetimes: 0
  viewnum: 69515
date: '2014-10-07 19:10:04'
editorchoice: false
excerpt: '本文以两台机器实现双集热备高可用集群，主机名node1的IP为192.168.122.168 ，主机名node2的IP为192.168.122.169
  。 一、安装集群软件 必须软件pcs，pacemaker，corosync，fence-agents-all，如果需要配置相关服务，也要安装对应的软件 二、配置防火墙
  1、禁止防火墙和selinux # systemctl disable firewalld # systemctl stop firewalld 修改/etc/sysconfig/selinux确保SELINUX=disabled，然后执行setenforce
  0或者reboot服务器以生效 2、设置防火墙规则 # firewall-cmd --permanent --add-service=high-availability
  # firewall-cmd --a'
fromurl: ''
id: 3963
islctt: false
largepic: /data/attachment/album/201410/07/191008vyq3zy00zmzyasyp.jpg
url: /article-3963-1.html
pic: /data/attachment/album/201410/07/191008vyq3zy00zmzyasyp.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: '本文以两台机器实现双集热备高可用集群，主机名node1的IP为192.168.122.168 ，主机名node2的IP为192.168.122.169
  。 一、安装集群软件 必须软件pcs，pacemaker，corosync，fence-agents-all，如果需要配置相关服务，也要安装对应的软件 二、配置防火墙
  1、禁止防火墙和selinux # systemctl disable firewalld # systemctl stop firewalld 修改/etc/sysconfig/selinux确保SELINUX=disabled，然后执行setenforce
  0或者reboot服务器以生效 2、设置防火墙规则 # firewall-cmd --permanent --add-service=high-availability
  # firewall-cmd --a'
tags:
- CentOS 7
- 集群
- Cluster
thumb: false
title: CentOS 7下搭建高可用集群
titlepic: false
translator: ''
updated: '2014-10-07 19:10:04'
---

![](/data/attachment/album/201410/07/191008vyq3zy00zmzyasyp.jpg)


本文以两台机器实现双集热备高可用集群，主机名node1的IP为192.168.122.168 ，主机名node2的IP为192.168.122.169 。


### 一、安装集群软件


必须软件pcs，pacemaker，corosync，fence-agents-all，如果需要配置相关服务，也要安装对应的软件


### 二、配置防火墙


#### 1、禁止防火墙和selinux



```
# systemctl disable firewalld
# systemctl stop firewalld
```

修改/etc/sysconfig/selinux确保SELINUX=disabled，然后执行setenforce 0或者reboot服务器以生效


#### 2、设置防火墙规则



```
# firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
```

###  三、各节点之间主机名互相解析


分别修改2台主机名分别为node1和node2，在centos 7中直接修改/etc/hostname加入本机主机名和主机表，然后重启网络服务即可。



```
#vi /etc/hostname
node1

#systemctl restart network.service
#hostname
node1
```

 配置2台主机的主机表，在/etc/hosts中加入



```
192.168.122.168 node1
192.168.122.169 node2
```

###  四、各节点之间时间同步


在node1和node2分别进行时间同步，可以使用ntp实现。 



```
[root@node1 ~]# ntpdate 172.16.0.1 //172.16.0.1 为时间服务器 
```

### 五、各节点之间配置ssh的无密码密钥访问。


下面的操作需要在各个节点上操作。



```
# ssh-keygen -t rsa -P ‘’   #这个生成一个密码为空的公钥和一个密钥，把公钥复制到对方节点上即可
# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2 #对方主机名用登录用户名
```

两台主机都要互相可以通信，所以两台主机都得互相生成密钥和复制公钥，相互的节点上的hosts文件是都要解析对方的主机名， 192.168.122.168 node1 192.168.122.169 node2 



```
# ssh node2 ‘date’;date #测试一下是否已经互信 
```

### 六、通过pacemaker来管理高可用集群


#### 1、创建集群用户


为了有利于各节点之间通信和配置集群，在每个节点上创建一个hacluster的用户，各个节点上的密码必须是同一个。



```
# passwd hacluster

Changing password for user hacluster.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.
```

#### 2、设置pcsd开机自启动



```
# systemctl start pcsd.service
# systemctl enable pcsd.service
```

#### 3、集群各节点之间进行认证



```
# pcs cluster auth node1 node2Username: hacluster Password: node1: Authorized node2: Authorized
```

4、创建并启动集群



```
[root@z1 ~]# pcs cluster setup --start --name my_cluster node1 node2

node1: Succeeded
node1: Starting Cluster...
node2: Succeeded
node2: Starting Cluster...
```

#### 5、设置集群自启动



```
# pcs cluster enable –all
```

6、查看集群状态信息



```
[root@z1 ~]# pcs cluster status
```

#### 7、设置fence设备


这个可以参考<Red Hat Enterprise Linux 7 High Availability Add-On Reference>


corosync默认启用了stonith，而当前集群并没有相应的stonith设备，因此此默 认配置目前尚不可用，这可以通过如下命令验证：



```
#crm_verify -L -V
```

可以通过如下面命令禁用stonith：



```
#pcs property set stonith-enabled=false（默认是true）
```

#### 8、配置存储


高可用集群既可以使用本地磁盘来构建纯软件的镜像型集群系统，也可以使用专门的共享磁盘装置来构建大规模的共享磁盘型集群系统，充分满足客户的不同需求。


共享磁盘主要有iscsi或DBRD。本文并没有使用共享磁盘。


#### 9、配置浮点IP


不管集群服务在哪运行,我们要一个固定的地址来提供服务。在这里我选择192.168.122.101作为浮动IP,给它取一个好记的名字 ClusterIP 并且告诉集群 每30秒检查它一次。



```
# pcs resource create VIP ocf:heartbeat:IPaddr2 ip=192.168.122.170 cidr_netmask=24 op monitor interval=30s
# pcs update VIP op monitor interval=15s
```

#### 10、配置apache服务


在node1和node2上安装httpd ，确认httpd开机被禁用 



```
# systemctl status httpd.service；
```

配置httpd监控页面（貌似不配置也可以通过systemd监控），分别在node1和node2上执行



```
# cat > /etc/httpd/conf.d/status.conf << EOF
SetHandler server-status
Order deny,allow
Deny from all
Allow from localhost
EOF
```

首先我们为Apache创建一个主页。在centos上面默认的Apache docroot是/var/www/html,所以我们在这个目录下面建立一个主页。


node1节点修改如下：



```
[root@node1 ~]# cat <<-END >/var/www/html/index.html
<html>
<body>Hello node1</body>
</html>

END
```

node2节点修改如下：



```
[root@node2 ~]# cat <<-END >/var/www/html/index.html 
<html>
<body>Hello node2</body>
</html>

END
```

下面语句是将httpd作为资源添加到集群中：



```
#pcs resource create WEB apache configfile="/etc/httpd/conf/httpd.conf" statusurl="http://127.0.0.1/server-status"
```

#### 11、创建group


将VIP和WEB resource捆绑到这个group中，使之作为一个整体在集群中切换。（此配置为可选)



```
# pcs resource group add MyGroup VIP
# pcs resource group add MyGroup WEB
```

#### 12、配置服务启动顺序


以避免出现资源冲突，语法：(pcs resource group add的时候也可以根据加的顺序依次启动，此配置为可选)



```
# pcs constraint order [action] then [action]
# pcs constraint order start VIP then start WEB
```

#### 13、指定优先的 Location （此配置为可选)


Pacemaker 并不要求你机器的硬件配置是相同的,可能某些机器比另外的机器配置要好。这种状况下我们会希望设置:当某个节点可用时,资源就要跑在上面之类的规则。为了达到这个效果我们创建location约束。同样的,我们给他取一个描述性的名字(prefer-node1),指明我们想在上面跑WEB 这个服务,多想在上面跑(我们现在指定分值为50,但是在双节点的集群状态下,任何大于0的值都可以达到想要的效果),以及目标节点的名字:



```
# pcs constraint location WEB prefers node1=50
# pcs constraint location WEB prefers node2=45
```

这里指定分值越大，代表越想在对应的节点上运行。


#### 14、资源粘性（此配置为可选)


一些环境中会要求尽量避免资源在节点之间迁移。迁移资源通常意味着一段时间内无法提供服务，某些复杂的服务，比如Oracle数据库，这个时间可能会很长。


为了达到这个效果，Pacemaker 有一个叫做“资源粘性值”的概念，它能够控制一个服务(资源)有多想呆在它正在运行的节点上。 


Pacemaker为了达到最优分布各个资源的目的，默认设置这个值为0。我们可以为每个资源定义不同的粘性值，但一般来说，更改默认粘性值就够了。资源粘性表示资源是否倾向于留在当前节点，如果为正整数，表示倾向，负数则会离开，-inf表示负无穷，inf表示正无穷。



```
# pcs resource defaults resource-stickiness=100
```

 


### 常用命令汇总：


查看集群状态：#pcs status


查看集群当前配置：#pcs config


开机后集群自启动：#pcs cluster enable –all


启动集群：#pcs cluster start –all


查看集群资源状态：#pcs resource show


验证集群配置情况：#crm\_verify -L -V


测试资源配置：#pcs resource debug-start resource


设置节点为备用状态：#pcs cluster standby node1